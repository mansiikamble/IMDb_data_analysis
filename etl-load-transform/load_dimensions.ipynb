{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977458f4-d94c-4a7a-b9c0-2cadcf0d6fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries management\n",
    "import dlt\n",
    "# from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09985ce7-35e7-4564-be98-be922564bee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG `workspace`\")\n",
    "spark.sql(\"USE SCHEMA `imdb_data_analysis`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ad936a-bae1-4f04-ba5e-0738d7bdb10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load DIM_PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1d3955-d65c-47c5-a35e-206237186247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: dim_person\n",
    "# =============================================================================\n",
    "# Description: Person dimension containing cast and crew personnel\n",
    "# Source: silver_imdb_name_basics\n",
    "# Grain: One row per unique person (nconst)\n",
    "# Load Type: Batch (industry standard for dimensions)\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, count, first, abs, hash\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_person\",\n",
    "    comment=\"Gold layer: Person dimension - cast and crew personnel with surrogate keys\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_nconst\", \"nconst IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_name\", \"primary_name IS NOT NULL\")\n",
    "def gold_dim_person():\n",
    "    \n",
    "    # Read from Silver layer as batch (standard for dimensions)\n",
    "    df = spark.read.table(\"LIVE.silver_imdb_name_basics\")\n",
    "    \n",
    "    # Aggregate to one row per person\n",
    "    df_person = df.groupBy(\"nconst\").agg(\n",
    "        first(\"primaryName\").alias(\"primary_name\"),\n",
    "        first(\"birthYear\").alias(\"birth_year\"),\n",
    "        first(\"deathYear\").alias(\"death_year\"),\n",
    "        count(\"profession\").alias(\"profession_count\")\n",
    "    )\n",
    "    \n",
    "    # Generate surrogate key using hash (deterministic)\n",
    "    df_person = df_person.withColumn(\n",
    "        \"person_key\",\n",
    "        # abs(hash(col(\"nconst\")))\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # Add audit column\n",
    "    df_person = df_person.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Select final columns\n",
    "    df_final = df_person.select(\n",
    "        \"person_key\",\n",
    "        \"nconst\",\n",
    "        \"primary_name\",\n",
    "        \"birth_year\",\n",
    "        \"death_year\",\n",
    "        \"profession_count\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63951a6b-534b-4641-bebe-c52327517024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load DIM_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7eec6c3-a817-4365-bdab-3ad2267f5323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: dim_region\n",
    "# =============================================================================\n",
    "# Description: Region dimension containing geographic regions/countries\n",
    "# Source: iso_countries.csv (direct load from reference file)\n",
    "# Grain: One row per unique region code\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, abs, hash, upper, trim, lit\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_region\",\n",
    "    comment=\"Gold layer: Region dimension - geographic regions with surrogate keys\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_region_code\", \"region_code IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_region_name\", \"region_name IS NOT NULL\")\n",
    "def gold_dim_region():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Define path to reference CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    country_codes_path = \"/Volumes/workspace/imdb_data_analysis/datastore/iso_countries/iso_countries.csv\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read Country Codes reference CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(country_codes_path)\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Transform columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"region_code\", upper(trim(col(\"Code\"))))\n",
    "    df = df.withColumn(\"region_name\", trim(col(\"Description\")))\n",
    "    \n",
    "    # Filter out null/empty codes (there's one row with empty code for Namibia)\n",
    "    df = df.filter(\n",
    "        (col(\"region_code\").isNotNull()) & \n",
    "        (col(\"region_code\") != \"\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"region_key\",\n",
    "        # abs(hash(col(\"region_code\")))\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # Add audit column\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"region_key\",\n",
    "        \"region_code\",\n",
    "        \"region_name\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab288f9b-ea0c-4d4a-a287-e9bb196e6b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load DIM_LANGUAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27f7242-5d07-4f1a-a32c-0ecf33c504f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: dim_language\n",
    "# =============================================================================\n",
    "# Description: Language dimension containing languages\n",
    "# Source: ISO_Language_Name.csv (direct load from reference file)\n",
    "# Grain: One row per unique language code\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, abs, hash, lower, trim, lit\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_language\",\n",
    "    comment=\"Gold layer: Language dimension - languages with surrogate keys\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_language_code\", \"language_code IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_language_name\", \"language_name IS NOT NULL\")\n",
    "def gold_dim_language():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Define path to reference CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    language_codes_path = \"/Volumes/workspace/imdb_data_analysis/datastore/ISO_Language_Name/ISO_Language_Name.csv\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read Language Codes reference CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(language_codes_path)\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Transform columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"language_code\", lower(trim(col(\"`Set 1 (639-1)`\"))))\n",
    "    df = df.withColumn(\"language_name\", trim(col(\"`ISO Language Name`\")))\n",
    "    \n",
    "    # Filter out null/empty codes\n",
    "    df = df.filter(\n",
    "        (col(\"language_code\").isNotNull()) & \n",
    "        (col(\"language_code\") != \"\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"language_key\",\n",
    "        # abs(hash(col(\"language_code\")))\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # Add audit column\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"language_key\",\n",
    "        \"language_code\",\n",
    "        \"language_name\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae46062-9aef-4a00-b581-c825099a97be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load DIM_TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f3cec3-a792-4684-b918-a6bb7a9b2956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: dim_title\n",
    "# =============================================================================\n",
    "# Description: Title dimension containing movies, TV series, and episodes\n",
    "# Source: silver_imdb_title_basics (deduplicated)\n",
    "# Grain: One row per unique title (tconst)\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, first, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_title\",\n",
    "    comment=\"Gold layer: Title dimension - movies, TV series, episodes\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_tconst\", \"tconst IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_title\", \"primary_title IS NOT NULL\")\n",
    "def gold_dim_title():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read from Silver layer\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = spark.read.table(\"LIVE.silver_imdb_title_basics\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Deduplicate (silver has exploded genres - aggregate back to one row per tconst)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.groupBy(\"tconst\").agg(\n",
    "        first(\"title_category\").alias(\"title_category\"),\n",
    "        first(\"titleType\").alias(\"title_type\"),\n",
    "        first(\"primaryTitle\").alias(\"primary_title\"),\n",
    "        first(\"originalTitle\").alias(\"original_title\"),\n",
    "        first(\"isAdult\").alias(\"is_adult\"),\n",
    "        first(\"startYear\").alias(\"start_year\"),\n",
    "        first(\"endYear\").alias(\"end_year\"),\n",
    "        first(\"runtimeMinutes\").alias(\"runtime_minutes\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"title_key\",\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Add metadata and audit columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"source_system\", lit(\"silver_imdb_title_basics\"))\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns in schema order\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"title_key\",\n",
    "        \"tconst\",\n",
    "        \"title_category\",\n",
    "        \"title_type\",\n",
    "        \"primary_title\",\n",
    "        \"original_title\",\n",
    "        \"is_adult\",\n",
    "        \"start_year\",\n",
    "        \"end_year\",\n",
    "        \"runtime_minutes\",\n",
    "        \"source_system\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3d0c76-6725-482e-a5d5-a2b77443549d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load DIM_EPISODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c0b8a27-7bc7-4909-9cd3-4b7a7651436c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: dim_episode\n",
    "# =============================================================================\n",
    "# Description: Episode dimension (outrigger from dim_title) containing TV episode hierarchy\n",
    "# Source: silver_imdb_title_episode + dim_title (for key lookups)\n",
    "# Grain: One row per unique episode (tconst)\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_episode\",\n",
    "    comment=\"Gold layer: Episode dimension (outrigger) - TV episode hierarchy with parent series linkage\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_tconst\", \"tconst IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_parent_tconst\", \"parent_tconst IS NOT NULL\")\n",
    "def gold_dim_episode():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read from Silver layer\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = spark.read.table(\"LIVE.silver_imdb_title_episode\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read dim_title for key lookups\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_title = spark.read.table(\"LIVE.dim_title\").select(\n",
    "        col(\"tconst\").alias(\"lookup_tconst\"),\n",
    "        col(\"title_key\").alias(\"lookup_title_key\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Lookup title_key for episode (tconst)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.join(\n",
    "        df_title,\n",
    "        df[\"tconst\"] == df_title[\"lookup_tconst\"],\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"title_key\", col(\"lookup_title_key\")\n",
    "    ).drop(\"lookup_tconst\", \"lookup_title_key\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Lookup parent_title_key for parent series (parentTconst)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_parent = spark.read.table(\"LIVE.dim_title\").select(\n",
    "        col(\"tconst\").alias(\"parent_lookup_tconst\"),\n",
    "        col(\"title_key\").alias(\"parent_lookup_title_key\")\n",
    "    )\n",
    "    \n",
    "    df = df.join(\n",
    "        df_parent,\n",
    "        df[\"parentTconst\"] == df_parent[\"parent_lookup_tconst\"],\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"parent_title_key\", col(\"parent_lookup_title_key\")\n",
    "    ).drop(\"parent_lookup_tconst\", \"parent_lookup_title_key\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Rename columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumnRenamed(\"parentTconst\", \"parent_tconst\")\n",
    "    df = df.withColumnRenamed(\"seasonNumber\", \"season_number\")\n",
    "    df = df.withColumnRenamed(\"episodeNumber\", \"episode_number\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"episode_key\",\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Add metadata and audit columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"source_system\", lit(\"silver_imdb_title_episode\"))\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns in schema order\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"episode_key\",\n",
    "        \"tconst\",\n",
    "        \"title_key\",\n",
    "        \"parent_tconst\",\n",
    "        \"parent_title_key\",\n",
    "        \"season_number\",\n",
    "        \"episode_number\",\n",
    "        \"source_system\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_dimensions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
