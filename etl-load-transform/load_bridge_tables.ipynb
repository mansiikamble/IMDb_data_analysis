{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3bb2ce2-53b4-4e50-a7a7-447d940dd418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries management\n",
    "import dlt\n",
    "# from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4406460f-6bea-4654-87d2-b05d6c6b2c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG `workspace`\")\n",
    "spark.sql(\"USE SCHEMA `imdb_data_analysis`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d461ce5-e1a5-433a-8a43-37f727d6ab1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load BRIDGE_TITLE_GENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e421dd1-1ee8-417e-be6c-599a383e400a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: bridge_title_genre\n",
    "# =============================================================================\n",
    "# Description: Bridge table linking titles to genres (many-to-many)\n",
    "# Source: silver_imdb_title_basics (exploded genre) + dim_title\n",
    "# Grain: One row per title-genre combination\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bridge_title_genre\",\n",
    "    comment=\"Gold layer: Bridge table linking titles to genres\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_title_key\", \"title_key IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_genre_name\", \"genre_name IS NOT NULL\")\n",
    "def gold_bridge_title_genre():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read from Silver layer (already has exploded genres)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = spark.read.table(\"LIVE.silver_imdb_title_basics\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select only tconst and genre, filter out Unknown genres\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.select(\"tconst\", \"genre\").distinct()\n",
    "    \n",
    "    df = df.filter(\n",
    "        (col(\"genre\").isNotNull()) &\n",
    "        (col(\"genre\") != \"Unknown\") &\n",
    "        (col(\"genre\") != \"\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read dim_title for key lookup\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_title = spark.read.table(\"LIVE.dim_title\").select(\n",
    "        col(\"tconst\").alias(\"title_tconst\"),\n",
    "        col(\"title_key\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join with dim_title\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.join(\n",
    "        df_title,\n",
    "        df[\"tconst\"] == df_title[\"title_tconst\"],\n",
    "        \"inner\"\n",
    "    ).drop(\"title_tconst\", \"tconst\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Rename columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumnRenamed(\"genre\", \"genre_name\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key (starting from 1)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"title_genre_key\",\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Add metadata and audit columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"source_system\", lit(\"silver_imdb_title_basics\"))\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns in schema order\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"title_genre_key\",\n",
    "        \"title_key\",\n",
    "        \"genre_name\",\n",
    "        \"source_system\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5417c9ba-fb0f-434d-92fb-ae0ad7c532df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load BRIDGE_TITLE_CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02fa45f2-42fc-4d1d-b61f-5555828e2a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: bridge_title_characters\n",
    "# =============================================================================\n",
    "# Description: Bridge table linking titles to characters played by actors\n",
    "# Source: silver_imdb_title_principals (filtered for character data only)\n",
    "# Grain: One row per title-person-character combination\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bridge_title_characters\",\n",
    "    comment=\"Gold layer: Bridge table linking titles to characters played by actors\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_title_key\", \"title_key IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_person_key\", \"person_key IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_characters\", \"characters IS NOT NULL\")\n",
    "def gold_bridge_title_characters():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read dimension tables for key lookups\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_title = spark.read.table(\"LIVE.dim_title\").select(\n",
    "        col(\"tconst\").alias(\"title_tconst\"),\n",
    "        col(\"title_key\")\n",
    "    )\n",
    "    \n",
    "    df_person = spark.read.table(\"LIVE.dim_person\").select(\n",
    "        col(\"nconst\").alias(\"person_nconst\"),\n",
    "        col(\"person_key\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read from Silver layer\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = spark.read.table(\"LIVE.silver_imdb_title_principals\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Filter: Only rows with character information\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.filter(\n",
    "        (col(\"characters\").isNotNull()) &\n",
    "        (col(\"characters\") != \"Unknown\") &\n",
    "        (col(\"characters\") != \"\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join with dim_title\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.join(\n",
    "        df_title,\n",
    "        df[\"tconst\"] == df_title[\"title_tconst\"],\n",
    "        \"inner\"\n",
    "    ).drop(\"title_tconst\", \"tconst\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join with dim_person\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.join(\n",
    "        df_person,\n",
    "        df[\"nconst\"] == df_person[\"person_nconst\"],\n",
    "        \"inner\"\n",
    "    ).drop(\"person_nconst\", \"nconst\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key (starting from 1)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\n",
    "        \"title_character_key\",\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Add metadata and audit columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = df.withColumn(\"source_system\", lit(\"silver_imdb_title_principals\"))\n",
    "    df = df.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns in schema order\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df.select(\n",
    "        \"title_character_key\",\n",
    "        \"title_key\",\n",
    "        \"person_key\",\n",
    "        \"ordering\",\n",
    "        \"category\",\n",
    "        \"characters\",\n",
    "        \"source_system\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfd78e3-edfb-4404-8b56-68fb0c91b7db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load BRIDGE_PERSON_PROFESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37f9bfa-0f77-4832-84ce-20c57c3d5039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLD LAYER: bridge_person_profession\n",
    "# =============================================================================\n",
    "# Description: Bridge table linking persons to their professions on titles\n",
    "# Source: silver_imdb_title_principals + silver_imdb_title_crew + dim_title + dim_person\n",
    "# Grain: One row per person-title-profession combination\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, monotonically_increasing_id, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bridge_person_profession\",\n",
    "    comment=\"Gold layer: Bridge table linking persons to their professions on titles\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"layer\": \"gold\",\n",
    "        \"domain\": \"imdb\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_person_key\", \"person_key IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_title_key\", \"title_key IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_profession_name\", \"profession_name IS NOT NULL\")\n",
    "def gold_bridge_person_profession():\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Read dimension tables for key lookups\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_title = spark.read.table(\"LIVE.dim_title\").select(\n",
    "        col(\"tconst\").alias(\"title_tconst\"),\n",
    "        col(\"title_key\")\n",
    "    )\n",
    "    \n",
    "    df_person = spark.read.table(\"LIVE.dim_person\").select(\n",
    "        col(\"nconst\").alias(\"person_nconst\"),\n",
    "        col(\"person_key\")\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Source 1: title.principals (has job details)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_principals = spark.read.table(\"LIVE.silver_imdb_title_principals\")\n",
    "    \n",
    "    df_principals = df_principals.select(\n",
    "        col(\"tconst\"),\n",
    "        col(\"nconst\"),\n",
    "        col(\"category\").alias(\"profession\"),\n",
    "        col(\"job\"),\n",
    "        lit(1).alias(\"source_priority\")  # Higher priority (has job info)\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Source 2: title.crew (directors and writers only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_crew = spark.read.table(\"LIVE.silver_imdb_title_crew\")\n",
    "    \n",
    "    df_crew = df_crew.select(\n",
    "        col(\"tconst\"),\n",
    "        col(\"nconst\"),\n",
    "        col(\"role\").alias(\"profession\"),\n",
    "        lit(\"Unknown\").alias(\"job\"),\n",
    "        lit(0).alias(\"source_priority\")  # Lower priority (no job info)\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Union both sources\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_merged = df_principals.unionByName(df_crew)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Deduplicate: Keep row with job info (principals) over Unknown (crew)\n",
    "    # -------------------------------------------------------------------------\n",
    "    window_spec = Window.partitionBy(\"tconst\", \"nconst\", \"profession\").orderBy(col(\"source_priority\").desc())\n",
    "    \n",
    "    df_dedup = df_merged.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "    df_dedup = df_dedup.filter(col(\"row_num\") == 1).drop(\"row_num\", \"source_priority\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join with dim_title\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dedup = df_dedup.join(\n",
    "        df_title,\n",
    "        df_dedup[\"tconst\"] == df_title[\"title_tconst\"],\n",
    "        \"inner\"\n",
    "    ).drop(\"title_tconst\", \"tconst\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join with dim_person\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dedup = df_dedup.join(\n",
    "        df_person,\n",
    "        df_dedup[\"nconst\"] == df_person[\"person_nconst\"],\n",
    "        \"inner\"\n",
    "    ).drop(\"person_nconst\", \"nconst\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Rename columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dedup = df_dedup.withColumnRenamed(\"profession\", \"profession_name\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate surrogate key (starting from 1)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dedup = df_dedup.withColumn(\n",
    "        \"person_profession_key\",\n",
    "        monotonically_increasing_id() + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Add metadata and audit columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dedup = df_dedup.withColumn(\"source_system\", lit(\"silver_imdb_title_principals,silver_imdb_title_crew\"))\n",
    "    df_dedup = df_dedup.withColumn(\"etl_load_timestamp\", current_timestamp())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Select final columns in schema order\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_final = df_dedup.select(\n",
    "        \"person_profession_key\",\n",
    "        \"person_key\",\n",
    "        \"title_key\",\n",
    "        \"profession_name\",\n",
    "        \"job\",\n",
    "        \"source_system\",\n",
    "        \"etl_load_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_bridge_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
