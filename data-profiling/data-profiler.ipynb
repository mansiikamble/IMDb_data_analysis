{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae1bc89-ae71-44a7-9d83-cc29031971cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe5fff0-9856-4930-ada9-3a23b5301f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import WorkspaceFileChecksStorageConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ws = WorkspaceClient()\n",
    "profiler = DQProfiler(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac1ecbe-847c-4ddb-aa1f-aa3c7eae302a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Base path for IMDB data in Unity Catalog Volume\n",
    "BASE_PATH = \"/Volumes/workspace/imdb_data_analysis/datastore\"\n",
    "\n",
    "# Define all 7 IMDB file paths\n",
    "file_paths = {\n",
    "    \"title_basics\": f\"{BASE_PATH}/title.basics.tsv\",\n",
    "    \"title_ratings\": f\"{BASE_PATH}/title.ratings.tsv\",\n",
    "    \"name_basics\": f\"{BASE_PATH}/name.basics.tsv\",\n",
    "    \"title_principals\": f\"{BASE_PATH}/title.principals.tsv\",\n",
    "    \"title_crew\": f\"{BASE_PATH}/title.crew.tsv\",\n",
    "    \"title_episode\": f\"{BASE_PATH}/title.episode.tsv\",\n",
    "    \"title_akas\": f\"{BASE_PATH}/title.akas.tsv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b66a4e-479e-425a-a14c-33ea8467e351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load title.basics\n",
    "df_title_basics = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_basics\"])\n",
    "\n",
    "# Load title.ratings\n",
    "df_title_ratings = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_ratings\"])\n",
    "\n",
    "# Load name.basics\n",
    "df_name_basics = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"name_basics\"])\n",
    "\n",
    "# Load title.principals\n",
    "df_title_principals = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_principals\"])\n",
    "\n",
    "# Load title.crew\n",
    "df_title_crew = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_crew\"])\n",
    "\n",
    "# Load title.episode\n",
    "df_title_episode = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_episode\"])\n",
    "\n",
    "# Load title.akas\n",
    "df_title_akas = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_akas\"])\n",
    "\n",
    "# Store in dictionary for easier iteration\n",
    "datasets = {\n",
    "    \"title_basics\": df_title_basics,\n",
    "    \"title_ratings\": df_title_ratings,\n",
    "    \"name_basics\": df_name_basics,\n",
    "    \"title_principals\": df_title_principals,\n",
    "    \"title_crew\": df_title_crew,\n",
    "    \"title_episode\": df_title_episode,\n",
    "    \"title_akas\": df_title_akas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3be82f3-1b2d-428e-867b-1db24da05b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Profiling configurations for each dataset (full data, no sampling)\n",
    "profiling_configs = {\n",
    "    \"title_basics\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 20,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.15,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"runtimeMinutes\", \"startYear\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.01\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"titleType\", \"primaryTitle\", \"isAdult\", \"startYear\", \"endYear\", \"runtimeMinutes\", \"genres\"]\n",
    "    },\n",
    "    \n",
    "    \"title_ratings\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 100,\n",
    "            \"distinct_ratio\": 0.01,\n",
    "            \"max_null_ratio\": 0.0,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"numVotes\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": False,\n",
    "            \"max_empty_ratio\": 0.0\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"averageRating\", \"numVotes\"]\n",
    "    },\n",
    "    \n",
    "    \"name_basics\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 50,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.25,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"birthYear\", \"deathYear\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.01\n",
    "        },\n",
    "        \"columns\": [\"nconst\", \"primaryName\", \"birthYear\", \"deathYear\", \"primaryProfession\", \"knownForTitles\"]\n",
    "    },\n",
    "    \n",
    "    \"title_principals\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 15,\n",
    "            \"distinct_ratio\": 0.0001,\n",
    "            \"max_null_ratio\": 0.5,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"ordering\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.3\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"ordering\", \"nconst\", \"category\", \"job\", \"characters\"]\n",
    "    },\n",
    "    \n",
    "    \"title_crew\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 20,\n",
    "            \"distinct_ratio\": 0.01,\n",
    "            \"max_null_ratio\": 0.4,\n",
    "            \"remove_outliers\": False,\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.35\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"directors\", \"writers\"]\n",
    "    },\n",
    "    \n",
    "    \"title_episode\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 30,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.3,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"seasonNumber\", \"episodeNumber\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.25\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"parentTconst\", \"seasonNumber\", \"episodeNumber\"]\n",
    "    },\n",
    "    \n",
    "    \"title_akas\": {\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 100,\n",
    "            \"distinct_ratio\": 0.0001,\n",
    "            \"max_null_ratio\": 0.5,\n",
    "            \"remove_outliers\": False,\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.4\n",
    "        },\n",
    "        \"columns\": [\"titleId\", \"ordering\", \"title\", \"region\", \"language\", \"types\", \"attributes\", \"isOriginalTitle\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3926aaa9-1f39-4d87-9a8e-eef2353757e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Profile all datasets with DQX\n",
    "all_profiles = {}\n",
    "all_summary_stats = {}\n",
    "profiling_times = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    config = profiling_configs[dataset_name]\n",
    "    \n",
    "    # Select only specified columns\n",
    "    df_filtered = df.select(config['columns'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Profile dataset\n",
    "    summary_stats, profiles = profiler.profile(\n",
    "        df=df_filtered,\n",
    "        options=config['options']\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    all_summary_stats[dataset_name] = summary_stats\n",
    "    all_profiles[dataset_name] = profiles\n",
    "    profiling_times[dataset_name] = elapsed_time\n",
    "    \n",
    "    print(f\"{dataset_name}: {len(profiles)} rules in {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb0018a-a5d0-4fe9-9df2-17bad0004574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display generated profiles for each dataset\n",
    "for dataset_name, profiles in all_profiles.items():\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"{dataset_name.upper()} - {len(profiles)} Rules\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    for i, profile in enumerate(profiles, 1):\n",
    "        print(f\"{i}. {profile}\")\n",
    "    \n",
    "    print(f\"\\nSummary Stats:\")\n",
    "    print(json.dumps(all_summary_stats[dataset_name], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01072678-727b-46a8-b85d-ae43cb011fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate quality checks from profiles\n",
    "generator = DQGenerator(ws)\n",
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "# Base path for saving checks\n",
    "base_checks_path = \"/Workspace/Shared/imdb_dqx/checks\"\n",
    "\n",
    "all_checks = {}\n",
    "\n",
    "for dataset_name, profiles in all_profiles.items():\n",
    "    # Generate quality rules\n",
    "    checks = generator.generate_dq_rules(profiles)\n",
    "    all_checks[dataset_name] = checks\n",
    "    \n",
    "    # Save checks to file\n",
    "    checks_file = f\"{base_checks_path}/{dataset_name}_checks.yml\"\n",
    "    dq_engine.save_checks(\n",
    "        checks,\n",
    "        config=WorkspaceFileChecksStorageConfig(location=checks_file)\n",
    "    )\n",
    "    \n",
    "    print(f\"{dataset_name}: {len(checks)} checks saved to {checks_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b183211-3f5b-4928-8c62-a5acbcdfaa7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate quality checks from profiles\n",
    "generator = DQGenerator(ws)\n",
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "# Base path for saving checks\n",
    "base_checks_path = \"/Workspace/Shared/imdb_dqx/checks\"\n",
    "\n",
    "all_checks = {}\n",
    "\n",
    "for dataset_name, profiles in all_profiles.items():\n",
    "    # Generate quality rules\n",
    "    checks = generator.generate_dq_rules(profiles)\n",
    "    all_checks[dataset_name] = checks\n",
    "    \n",
    "    # Save checks to file\n",
    "    checks_file = f\"{base_checks_path}/{dataset_name}_checks.yml\"\n",
    "    dq_engine.save_checks(\n",
    "        checks,\n",
    "        config=WorkspaceFileChecksStorageConfig(location=checks_file)\n",
    "    )\n",
    "    \n",
    "    print(f\"{dataset_name}: {len(checks)} checks saved to {checks_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02112864-3a6b-475e-ad9d-d51021dffb3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create comprehensive profiling summary\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    summary_data.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Total Records': f\"{datasets[dataset_name].count():,}\",\n",
    "        'Columns Profiled': len(profiling_configs[dataset_name]['columns']),\n",
    "        'Quality Rules': len(all_profiles[dataset_name]),\n",
    "        'Profiling Time (s)': f\"{profiling_times[dataset_name]:.2f}\",\n",
    "        'Key Issues': f\"{all_summary_stats[dataset_name][profiling_configs[dataset_name]['columns'][0]]['count']:,} records analyzed\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(f\"\\nTotal Profiling Time: {sum(profiling_times.values()):.2f} seconds ({sum(profiling_times.values())/60:.2f} minutes)\")\n",
    "print(f\"Total Quality Rules Generated: {sum(len(p) for p in all_profiles.values())}\")\n",
    "print(f\"Quality Checks Saved To: {base_checks_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e084e4-bb56-4dba-bf69-27ef866a28a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load ISO language and country reference files\n",
    "df_languages = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/workspace/imdb_data_analysis/datastore/iso_languages.csv\")\n",
    "\n",
    "df_countries = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/workspace/imdb_data_analysis/datastore/iso_countries.csv\")\n",
    "\n",
    "print(\"ISO Languages:\")\n",
    "display(df_languages.limit(20))\n",
    "\n",
    "print(\"\\nISO Countries:\")\n",
    "display(df_countries.limit(20))\n",
    "\n",
    "print(f\"\\nLanguages: {df_languages.count()} records\")\n",
    "print(f\"Countries: {df_countries.count()} records\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-profiler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
