{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2711e3-2f45-486c-9766-f8bfca5733f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install databricks-labs-dqx --quiet\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466bc4fd-3af6-4d54-b9fa-4db7c125f6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import WorkspaceFileChecksStorageConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, countDistinct, when, isnan, sum as _sum, avg, min as _min, max as _max, stddev\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada509f4-7dd0-4be5-bd72-bd9464e41178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Initialize\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ws = WorkspaceClient()\n",
    "profiler = DQProfiler(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6791b4ce-ba77-4a89-9857-cb6c1e5dbb4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(\"USE CATALOG `workspace`\")\n",
    "spark.sql(\"USE SCHEMA `imdb_data_analysis`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8d383c-aba4-4eb7-b9e9-da5fcf00e5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 2. Configuration and File Paths\n",
    "# Base path for IMDB data in Unity Catalog Volume\n",
    "BASE_PATH = \"/Volumes/workspace/imdb_data_analysis/datastore\"\n",
    "\n",
    "# Define all 7 IMDB file paths (TSV files inside folders)\n",
    "file_paths = {\n",
    "    \"title_basics\": f\"{BASE_PATH}/title.basics/title.basics.tsv\",\n",
    "    \"title_ratings\": f\"{BASE_PATH}/title.ratings/title.ratings.tsv\",\n",
    "    \"name_basics\": f\"{BASE_PATH}/name.basics/name.basics.tsv\",\n",
    "    \"title_principals\": f\"{BASE_PATH}/title.principals/title.principals.tsv\",\n",
    "    \"title_crew\": f\"{BASE_PATH}/title.crew/title.crew.tsv\",\n",
    "    \"title_episode\": f\"{BASE_PATH}/title.episode/title.episode.tsv\",\n",
    "    \"title_akas\": f\"{BASE_PATH}/title.akas/title.akas.tsv\"\n",
    "}\n",
    "\n",
    "# Reference data paths \n",
    "iso_language_path = f\"{BASE_PATH}/ISO_Language_Name/ISO_Language_Name.csv\"\n",
    "iso_country_path = f\"{BASE_PATH}/iso_countries/iso_countries.csv\"\n",
    "\n",
    "# Output path for quality checks\n",
    "OUTPUT_CHECKS_PATH = \"/Workspace/Shared/imdb_dqx/checks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783588a4-0693-404b-b124-6ac8277b5cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#3. Load All Datasets\n",
    "# Load title.basics\n",
    "df_title_basics = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_basics\"])\n",
    "\n",
    "\n",
    "# Load title.ratings\n",
    "df_title_ratings = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_ratings\"])\n",
    "\n",
    "\n",
    "# Load name.basics\n",
    "df_name_basics = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"name_basics\"])\n",
    "\n",
    "\n",
    "# Load title.principals\n",
    "df_title_principals = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_principals\"])\n",
    "\n",
    "\n",
    "# Load title.crew\n",
    "df_title_crew = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_crew\"])\n",
    "\n",
    "\n",
    "# Load title.episode\n",
    "df_title_episode = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_episode\"])\n",
    "\n",
    "# Load title.akas\n",
    "df_title_akas = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"quote\", \"\") \\\n",
    "    .option(\"escape\", \"\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .csv(file_paths[\"title_akas\"])\n",
    "\n",
    "\n",
    "# Store in dictionary for easier iteration\n",
    "datasets = {\n",
    "    \"title_basics\": df_title_basics,\n",
    "    \"title_ratings\": df_title_ratings,\n",
    "    \"name_basics\": df_name_basics,\n",
    "    \"title_principals\": df_title_principals,\n",
    "    \"title_crew\": df_title_crew,\n",
    "    \"title_episode\": df_title_episode,\n",
    "    \"title_akas\": df_title_akas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463b94cd-9467-474f-8259-8712369022fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 4. Initial Data Exploration\n",
    "\n",
    "def explore_dataset(df, name):\n",
    "    \"\"\"Quick exploration of dataset structure\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Records: {df.count():,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    print(f\"\\nSample Data:\")\n",
    "    display(df.limit(5))\n",
    "    \n",
    "    # Null count summary\n",
    "    null_counts = df.select([\n",
    "        _sum(when(col(c).isNull() | (col(c) == \"\"), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "    \n",
    "    total_records = df.count()\n",
    "    print(f\"\\nNull/Empty Analysis:\")\n",
    "    for col_name, null_count in null_counts.items():\n",
    "        null_pct = (null_count / total_records) * 100\n",
    "        if null_pct > 0:\n",
    "            print(f\"  {col_name}: {null_count:,} ({null_pct:.2f}%)\")\n",
    "\n",
    "# Explore each dataset\n",
    "for name, df in datasets.items():\n",
    "    explore_dataset(df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0d751b-fde9-4014-894b-7ca44e5fc956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 5. Profiling Configurations\n",
    "\n",
    "profiling_configs = {\n",
    "    \"title_basics\": {\n",
    "        \"description\": \"Core title master data - Movies, TV shows, episodes\",\n",
    "        \"business_critical_columns\": [\"tconst\", \"titleType\", \"primaryTitle\", \"startYear\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 20,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.15,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"runtimeMinutes\", \"startYear\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.01\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"titleType\", \"primaryTitle\", \"originalTitle\", \"isAdult\", \"startYear\", \"endYear\", \"runtimeMinutes\", \"genres\"]\n",
    "    },\n",
    "    \n",
    "    \"title_ratings\": {\n",
    "        \"description\": \"Rating and voting metrics for titles\",\n",
    "        \"business_critical_columns\": [\"tconst\", \"averageRating\", \"numVotes\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 100,\n",
    "            \"distinct_ratio\": 0.01,\n",
    "            \"max_null_ratio\": 0.0,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"numVotes\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": False,\n",
    "            \"max_empty_ratio\": 0.0\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"averageRating\", \"numVotes\"]\n",
    "    },\n",
    "    \n",
    "    \"name_basics\": {\n",
    "        \"description\": \"Person/talent master data\",\n",
    "        \"business_critical_columns\": [\"nconst\", \"primaryName\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 50,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.25,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"birthYear\", \"deathYear\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.01\n",
    "        },\n",
    "        \"columns\": [\"nconst\", \"primaryName\", \"birthYear\", \"deathYear\", \"primaryProfession\", \"knownForTitles\"]\n",
    "    },\n",
    "    \n",
    "    \"title_principals\": {\n",
    "        \"description\": \"Cast and crew role assignments\",\n",
    "        \"business_critical_columns\": [\"tconst\", \"nconst\", \"category\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 15,\n",
    "            \"distinct_ratio\": 0.0001,\n",
    "            \"max_null_ratio\": 0.5,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"ordering\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.3\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"ordering\", \"nconst\", \"category\", \"job\", \"characters\"]\n",
    "    },\n",
    "    \n",
    "    \"title_crew\": {\n",
    "        \"description\": \"Directors and writers\",\n",
    "        \"business_critical_columns\": [\"tconst\", \"directors\", \"writers\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 20,\n",
    "            \"distinct_ratio\": 0.01,\n",
    "            \"max_null_ratio\": 0.4,\n",
    "            \"remove_outliers\": False,\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.35\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"directors\", \"writers\"]\n",
    "    },\n",
    "    \n",
    "    \"title_episode\": {\n",
    "        \"description\": \"TV episode structure and hierarchy\",\n",
    "        \"business_critical_columns\": [\"tconst\", \"parentTconst\", \"seasonNumber\", \"episodeNumber\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 30,\n",
    "            \"distinct_ratio\": 0.001,\n",
    "            \"max_null_ratio\": 0.3,\n",
    "            \"remove_outliers\": True,\n",
    "            \"outlier_columns\": [\"seasonNumber\", \"episodeNumber\"],\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.25\n",
    "        },\n",
    "        \"columns\": [\"tconst\", \"parentTconst\", \"seasonNumber\", \"episodeNumber\"]\n",
    "    },\n",
    "    \n",
    "    \"title_akas\": {\n",
    "        \"description\": \"Regional title variations and localizations\",\n",
    "        \"business_critical_columns\": [\"titleId\", \"region\", \"language\", \"title\"],\n",
    "        \"options\": {\n",
    "            \"sample_fraction\": None,\n",
    "            \"limit\": None,\n",
    "            \"round\": True,\n",
    "            \"max_in_count\": 100,\n",
    "            \"distinct_ratio\": 0.0001,\n",
    "            \"max_null_ratio\": 0.5,\n",
    "            \"remove_outliers\": False,\n",
    "            \"num_sigmas\": 3,\n",
    "            \"trim_strings\": True,\n",
    "            \"max_empty_ratio\": 0.4\n",
    "        },\n",
    "        \"columns\": [\"titleId\", \"ordering\", \"title\", \"region\", \"language\", \"types\", \"attributes\", \"isOriginalTitle\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úì Profiling configurations defined for all 7 datasets\")\n",
    "for name, config in profiling_configs.items():\n",
    "    print(f\"  {name}: {len(config['columns'])} columns | {config['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63fb2d6-51b4-4528-b398-d302963dc0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 6. Execute Data Profiling\n",
    "\n",
    "# Profile all datasets with DQX\n",
    "all_profiles = {}\n",
    "all_summary_stats = {}\n",
    "profiling_times = {}\n",
    "profiling_errors = {}\n",
    "\n",
    "print(\"Starting comprehensive profiling...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    config = profiling_configs[dataset_name]\n",
    "    \n",
    "    print(f\"\\nüîç Profiling: {dataset_name}\")\n",
    "    print(f\"   Description: {config['description']}\")\n",
    "    print(f\"   Columns: {', '.join(config['columns'][:5])}{'...' if len(config['columns']) > 5 else ''}\")\n",
    "    \n",
    "    try:\n",
    "        # Select only specified columns\n",
    "        df_filtered = df.select(config['columns'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Profile dataset\n",
    "        summary_stats, profiles = profiler.profile(\n",
    "            df=df_filtered,\n",
    "            options=config['options']\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        all_summary_stats[dataset_name] = summary_stats\n",
    "        all_profiles[dataset_name] = profiles\n",
    "        profiling_times[dataset_name] = elapsed_time\n",
    "        \n",
    "        print(f\"   ‚úì Success: {len(profiles)} quality rules generated in {elapsed_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        profiling_errors[dataset_name] = str(e)\n",
    "        print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"  Total rules: {sum(len(p) for p in all_profiles.values())}\")\n",
    "print(f\"  Errors: {len(profiling_errors)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d759c251-2be0-46dc-8b54-562c9e81f068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 7. Profiling Results Summary\n",
    "\n",
    "# Create comprehensive profiling summary\n",
    "summary_data = []\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    if dataset_name in all_profiles:\n",
    "        config = profiling_configs[dataset_name]\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Description': config['description'],\n",
    "            'Total Records': f\"{datasets[dataset_name].count():,}\",\n",
    "            'Columns Profiled': len(config['columns']),\n",
    "            'Quality Rules': len(all_profiles[dataset_name]),\n",
    "            'Critical Columns': len(config['business_critical_columns']),\n",
    "            'Profiling Time (s)': f\"{profiling_times[dataset_name]:.2f}\",\n",
    "            'Status': '‚úì Success'\n",
    "        })\n",
    "    else:\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Description': profiling_configs[dataset_name]['description'],\n",
    "            'Total Records': f\"{datasets[dataset_name].count():,}\",\n",
    "            'Columns Profiled': 0,\n",
    "            'Quality Rules': 0,\n",
    "            'Critical Columns': 0,\n",
    "            'Profiling Time (s)': '0.00',\n",
    "            'Status': f'‚úó Error: {profiling_errors.get(dataset_name, \"Unknown\")}'\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af06543-8f2a-41a0-8277-d257cf75ffa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 8. Detailed Quality Rules by Dataset\n",
    "# Display generated profiles for each dataset\n",
    "for dataset_name, profiles in all_profiles.items():\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"üìã {dataset_name.upper()} - {len(profiles)} Quality Rules\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"Description: {profiling_configs[dataset_name]['description']}\")\n",
    "    print(f\"Business Critical: {', '.join(profiling_configs[dataset_name]['business_critical_columns'])}\")\n",
    "    print(f\"\\nGenerated Rules:\")\n",
    "    \n",
    "    for i, profile in enumerate(profiles, 1):\n",
    "        print(f\"  {i}. {profile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43051fad-1d7d-4e2c-98fe-c65db94b2fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#9. Data Quality Insights & Findings\n",
    "\n",
    "def analyze_data_quality(dataset_name, df, summary_stats, profiles):\n",
    "    \"\"\"Analyze data quality metrics and generate insights\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç DATA QUALITY ANALYSIS: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    config = profiling_configs[dataset_name]\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # 1. Completeness Analysis\n",
    "    print(f\"\\n1Ô∏è‚É£ COMPLETENESS ANALYSIS\")\n",
    "    print(f\"   Total Records: {total_records:,}\")\n",
    "    \n",
    "    for col_name in config['columns']:\n",
    "        if col_name in summary_stats:\n",
    "            stats = summary_stats[col_name]\n",
    "            null_count = stats.get('null_count', 0)\n",
    "            null_pct = (null_count / total_records) * 100 if total_records > 0 else 0\n",
    "            \n",
    "            status = \"‚úì\" if null_pct < 5 else \"‚ö†Ô∏è\" if null_pct < 20 else \"‚ùå\"\n",
    "            print(f\"   {status} {col_name}: {null_pct:.2f}% null ({null_count:,} records)\")\n",
    "    \n",
    "    # 2. Uniqueness Analysis\n",
    "    print(f\"\\n2Ô∏è‚É£ UNIQUENESS ANALYSIS\")\n",
    "    for col_name in config['business_critical_columns']:\n",
    "        if col_name in summary_stats:\n",
    "            stats = summary_stats[col_name]\n",
    "            distinct_count = stats.get('distinct_count', 0)\n",
    "            distinct_pct = (distinct_count / total_records) * 100 if total_records > 0 else 0\n",
    "            \n",
    "            # Check if key column\n",
    "            is_key = distinct_pct > 95\n",
    "            status = \"üîë\" if is_key else \"üìä\"\n",
    "            print(f\"   {status} {col_name}: {distinct_count:,} distinct ({distinct_pct:.2f}%)\")\n",
    "    \n",
    "    # 3. Quality Rules Summary\n",
    "    print(f\"\\n3Ô∏è‚É£ QUALITY RULES GENERATED\")\n",
    "    print(f\"   Total Rules: {len(profiles)}\")\n",
    "    \n",
    "    # Categorize rules\n",
    "    null_checks = [p for p in profiles if 'IS NOT NULL' in str(p) or 'null' in str(p).lower()]\n",
    "    range_checks = [p for p in profiles if 'BETWEEN' in str(p) or '>=' in str(p) or '<=' in str(p)]\n",
    "    pattern_checks = [p for p in profiles if 'RLIKE' in str(p) or 'IN' in str(p)]\n",
    "    \n",
    "    print(f\"   - Null/Completeness checks: {len(null_checks)}\")\n",
    "    print(f\"   - Range/Boundary checks: {len(range_checks)}\")\n",
    "    print(f\"   - Pattern/Format checks: {len(pattern_checks)}\")\n",
    "    print(f\"   - Other checks: {len(profiles) - len(null_checks) - len(range_checks) - len(pattern_checks)}\")\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name in all_profiles.keys():\n",
    "    analyze_data_quality(\n",
    "        dataset_name,\n",
    "        datasets[dataset_name],\n",
    "        all_summary_stats[dataset_name],\n",
    "        all_profiles[dataset_name]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe83b6b-5a2e-4a0b-91d5-ba56f3da9c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 10. Generate and Save Quality Checks\n",
    "# Generate quality checks from profiles\n",
    "\n",
    "generator = DQGenerator(ws)\n",
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "all_checks = {}\n",
    "checks_saved = {}\n",
    "\n",
    "print(\"Generating and saving quality checks...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, profiles in all_profiles.items():\n",
    "    try:\n",
    "        print(f\"\\nüìù {dataset_name}\")\n",
    "        \n",
    "        # Generate quality rules\n",
    "        checks = generator.generate_dq_rules(profiles)\n",
    "        all_checks[dataset_name] = checks\n",
    "        \n",
    "        # Save checks to file\n",
    "        checks_file = f\"{OUTPUT_CHECKS_PATH}/{dataset_name}_checks.yml\"\n",
    "        dq_engine.save_checks(\n",
    "            checks,\n",
    "            config=WorkspaceFileChecksStorageConfig(location=checks_file)\n",
    "        )\n",
    "        checks_saved[dataset_name] = checks_file\n",
    "        \n",
    "        print(f\"   ‚úì {len(checks)} checks saved to: {checks_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error saving checks: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Quality checks generated and saved\")\n",
    "print(f\"  Location: {OUTPUT_CHECKS_PATH}\")\n",
    "print(f\"  Total files: {len(checks_saved)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9531e4ab-75b0-4acb-8ab3-08170eb1cf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# MAGIC ## 11. ISO Reference Data Validation\n",
    "\n",
    "\n",
    "print(\"üåç ISO REFERENCE DATA VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Load ISO reference CSV files\n",
    "    df_languages = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(iso_language_path)\n",
    "    print(f\"‚úì ISO Languages loaded: {df_languages.count():,} records\")\n",
    "    \n",
    "    df_countries = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(iso_country_path)\n",
    "    print(f\"‚úì ISO Countries loaded: {df_countries.count():,} records\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nISO Languages Sample:\")\n",
    "    display(df_languages.limit(10))\n",
    "    \n",
    "    print(\"\\nISO Countries Sample:\")\n",
    "    display(df_countries.limit(10))\n",
    "    \n",
    "    # Show column names for validation\n",
    "    print(f\"\\nLanguage table columns: {df_languages.columns}\")\n",
    "    print(f\"Country table columns: {df_countries.columns}\")\n",
    "    \n",
    "    # Get distinct values from title_akas\n",
    "    print(f\"\\n1Ô∏è‚É£ Language codes in title_akas\")\n",
    "    akas_languages = datasets['title_akas'].select('language').distinct().filter(col('language').isNotNull())\n",
    "    total_distinct_langs = akas_languages.count()\n",
    "    print(f\"   Total distinct languages: {total_distinct_langs}\")\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ Region codes in title_akas\")\n",
    "    akas_regions = datasets['title_akas'].select('region').distinct().filter(col('region').isNotNull())\n",
    "    total_distinct_regions = akas_regions.count()\n",
    "    print(f\"   Total distinct regions: {total_distinct_regions}\")\n",
    "    \n",
    "    print(f\"\\n‚úì ISO reference data validation complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ISO reference data validation skipped: {str(e)}\")\n",
    "    print(\"   Ensure ISO reference files exist in the specified paths\")\n",
    "    print(f\"   Expected paths:\")\n",
    "    print(f\"   - Languages: {iso_language_path}\")\n",
    "    print(f\"   - Countries: {iso_country_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72797c8b-08ac-4c87-b714-600f25976fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 12. Multi-Value Field Analysis\n",
    "\n",
    "from pyspark.sql.functions import split, explode, size, trim\n",
    "\n",
    "print(\"üî¢ MULTI-VALUE FIELD ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Genres in title_basics\n",
    "print(\"\\n1Ô∏è‚É£ Genre Analysis (title_basics.genres)\")\n",
    "genres_df = datasets['title_basics'].select(\n",
    "    col('tconst'),\n",
    "    split(col('genres'), ',').alias('genre_array')\n",
    ").withColumn('genre_count', size(col('genre_array')))\n",
    "\n",
    "genres_with_data = genres_df.filter(col('genre_count') > 0)\n",
    "print(f\"   Records with genres: {genres_with_data.count():,}\")\n",
    "print(f\"   Average genres per title: {genres_df.agg(avg('genre_count')).collect()[0][0]:.2f}\")\n",
    "print(f\"   Max genres per title: {genres_df.agg(_max('genre_count')).collect()[0][0]}\")\n",
    "\n",
    "# Top genres\n",
    "top_genres = genres_df.select(explode(col('genre_array')).alias('genre')) \\\n",
    "    .filter(col('genre').isNotNull() & (col('genre') != '')) \\\n",
    "    .groupBy('genre') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc()) \\\n",
    "    .limit(15)\n",
    "print(f\"\\n   Top 15 Genres:\")\n",
    "display(top_genres)\n",
    "\n",
    "# 2. Primary Professions in name_basics\n",
    "print(\"\\n2Ô∏è‚É£ Profession Analysis (name_basics.primaryProfession)\")\n",
    "professions_df = datasets['name_basics'].select(\n",
    "    col('nconst'),\n",
    "    split(col('primaryProfession'), ',').alias('profession_array')\n",
    ").withColumn('profession_count', size(col('profession_array')))\n",
    "\n",
    "professions_with_data = professions_df.filter(col('profession_count') > 0)\n",
    "print(f\"   People with professions: {professions_with_data.count():,}\")\n",
    "print(f\"   Average professions per person: {professions_df.agg(avg('profession_count')).collect()[0][0]:.2f}\")\n",
    "print(f\"   Max professions per person: {professions_df.agg(_max('profession_count')).collect()[0][0]}\")\n",
    "\n",
    "# Top professions\n",
    "top_professions = professions_df.select(explode(col('profession_array')).alias('profession')) \\\n",
    "    .filter(col('profession').isNotNull() & (col('profession') != '')) \\\n",
    "    .groupBy('profession') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc()) \\\n",
    "    .limit(15)\n",
    "print(f\"\\n   Top 15 Professions:\")\n",
    "display(top_professions)\n",
    "\n",
    "# 3. Directors in title_crew\n",
    "print(\"\\n3Ô∏è‚É£ Directors Analysis (title_crew.directors)\")\n",
    "directors_df = datasets['title_crew'].filter(col('directors').isNotNull()) \\\n",
    "    .select(\n",
    "        col('tconst'),\n",
    "        split(col('directors'), ',').alias('director_array')\n",
    "    ).withColumn('director_count', size(col('director_array')))\n",
    "\n",
    "print(f\"   Titles with directors: {directors_df.count():,}\")\n",
    "print(f\"   Average directors per title: {directors_df.agg(avg('director_count')).collect()[0][0]:.2f}\")\n",
    "print(f\"   Max directors per title: {directors_df.agg(_max('director_count')).collect()[0][0]}\")\n",
    "\n",
    "# 4. Writers in title_crew\n",
    "print(\"\\n4Ô∏è‚É£ Writers Analysis (title_crew.writers)\")\n",
    "writers_df = datasets['title_crew'].filter(col('writers').isNotNull()) \\\n",
    "    .select(\n",
    "        col('tconst'),\n",
    "        split(col('writers'), ',').alias('writer_array')\n",
    "    ).withColumn('writer_count', size(col('writer_array')))\n",
    "\n",
    "print(f\"   Titles with writers: {writers_df.count():,}\")\n",
    "print(f\"   Average writers per title: {writers_df.agg(avg('writer_count')).collect()[0][0]:.2f}\")\n",
    "print(f\"   Max writers per title: {writers_df.agg(_max('writer_count')).collect()[0][0]}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Multi-value field analysis complete\")\n",
    "print(f\"\\nüí° Insights:\")\n",
    "print(f\"   - Genres: Useful for BRIDGE_Title_Genre table design\")\n",
    "print(f\"   - Professions: Useful for BRIDGE_Person_Profession table design\")\n",
    "print(f\"   - Directors/Writers: Will be normalized in FACT_Title_Person_Role\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-profiling",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
